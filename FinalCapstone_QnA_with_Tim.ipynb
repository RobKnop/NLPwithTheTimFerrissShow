{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/robert/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "pd.set_option('display.max_rows', 400)\n",
    "pd.set_option('float_format', '{:.2f}'.format)\n",
    "pd.set_option('isplay.max_colwidth', 300)\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "import IPython\n",
    "from IPython import get_ipython\n",
    "get_ipython().run_line_magic('matplotlib', 'inline')\n",
    "from sklearn.model_selection import train_test_split\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from sklearn.decomposition import PCA, TruncatedSVD\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "import spacy\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import Normalizer\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn import metrics\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FINAL CAPSTONE PROJECT\n",
    "# Doing NLP with the Tim Ferriss Podcast\n",
    "## Topic Modeling, Clustering and QnA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data set \n",
    "* Scraping https://tim.blog/2018/09/20/all-transcripts-from-the-tim-ferriss-show/ with the Python package BeautifulSoup\n",
    "* Output: csv file with all podcast episodes of number 151 - 375\n",
    "* all podcast of number before 151 do not have a HTML page\n",
    "\n",
    "### Exploring the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>title</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>151</td>\n",
       "      <td>Caroline Paul (#151)</td>\n",
       "      <td>'Maria said that Caroline is all about living courageously and embracing adventure in our culture of safe achievement. That is music to my ears. I feel like we’ve grown soft and weak, and it is time to remedy that with habits, practices, and stories that impart lessons.''On top of that, Caroline...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>152</td>\n",
       "      <td>On Philosophy and Riches (#152)</td>\n",
       "      <td>'And Seneca takes a little while with his preamble to get warmed up, but my favorite portion begins with: I might close my letter at this point. So you can use that as a bookmark. I might close my letter at this point. And I was talking to a friend of mine about Seneca. He’s a huge fan, also. An...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>153</td>\n",
       "      <td>BJ Miller (#153)</td>\n",
       "      <td>'Well, it turns out, BJ Miller, M.D., Dr. Miller, knows exactly this. BJ is a palliative care physician at Zen Hospice Project in San Francisco where he thinks deeply about how to create a dignified, graceful end of life for his patients. Now, this is, of course, not one of the usual suspects yo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>154</td>\n",
       "      <td>Paulo Coelho (#154)</td>\n",
       "      <td>'His books of near universal appeal span from The Alchemist to the most recent, Adultery. And his work has been translated into more than 70 languages. Who knew there were even 70 languages on the planet? And few people know that The Alchemist, which was sold to an original Brazilian publisher, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>155</td>\n",
       "      <td>On Zero-to-Hero Transformations (#155)</td>\n",
       "      <td>'Instead, I am answering questions that you all or I should say many of you, wanted me to answer.  There have been requests for me to do a Q&amp;A, and I went onto the Facebook, www.Facebook.com/timferriss, and linked to a Reddit post where I had people submit and up-vote questions.  At least 100 of...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    id                                   title  \\\n",
       "0  151                    Caroline Paul (#151)   \n",
       "1  152         On Philosophy and Riches (#152)   \n",
       "2  153                        BJ Miller (#153)   \n",
       "3  154                     Paulo Coelho (#154)   \n",
       "4  155  On Zero-to-Hero Transformations (#155)   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                          text  \n",
       "0  'Maria said that Caroline is all about living courageously and embracing adventure in our culture of safe achievement. That is music to my ears. I feel like we’ve grown soft and weak, and it is time to remedy that with habits, practices, and stories that impart lessons.''On top of that, Caroline...  \n",
       "1  'And Seneca takes a little while with his preamble to get warmed up, but my favorite portion begins with: I might close my letter at this point. So you can use that as a bookmark. I might close my letter at this point. And I was talking to a friend of mine about Seneca. He’s a huge fan, also. An...  \n",
       "2  'Well, it turns out, BJ Miller, M.D., Dr. Miller, knows exactly this. BJ is a palliative care physician at Zen Hospice Project in San Francisco where he thinks deeply about how to create a dignified, graceful end of life for his patients. Now, this is, of course, not one of the usual suspects yo...  \n",
       "3  'His books of near universal appeal span from The Alchemist to the most recent, Adultery. And his work has been translated into more than 70 languages. Who knew there were even 70 languages on the planet? And few people know that The Alchemist, which was sold to an original Brazilian publisher, ...  \n",
       "4  'Instead, I am answering questions that you all or I should say many of you, wanted me to answer.  There have been requests for me to do a Q&A, and I went onto the Facebook, www.Facebook.com/timferriss, and linked to a Reddit post where I had people submit and up-vote questions.  At least 100 of...  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import raw data\n",
    "# This data set contains the transcripts of Tim Ferriss Podcasts\n",
    "# Source: https://tim.blog/2018/09/20/all-transcripts-from-the-tim-ferriss-show/\n",
    "df = pd.read_csv(\"raw/transcript-151-3XX.csv\")\n",
    "df.columns=['id', 'title', 'text']\n",
    "df.head()\n",
    "\n",
    "# Cleaning the titles\n",
    "df['title'] = df['title'].str.replace(r\"The Tim Ferriss Show Transcripts: \", \"\")\n",
    "df['title'] = df['title'].str.replace(r\"Transcripts: \", \"\")\n",
    "df['title'] = df['title'].str.replace(r\"Tim Ferriss Show Transcript: \", \"\")\n",
    "df['title'] = df['title'].str.replace(r\"Episode \", \"\")\n",
    "\n",
    "# Cleaning the transcript text\n",
    "def clean_text(df, text_field):\n",
    "    df[text_field] = df[text_field].str.replace(r\"\\\\n\\\\n'\", \"\")\n",
    "    df[text_field] = df[text_field].str.replace(r\"\\\\n\", \" \")\n",
    "    df[text_field] = df[text_field].str.replace(r\"\\\\xa0\", \" \")\n",
    "    df[text_field] = df[text_field].str.replace(r\"xa0\", \" \")\n",
    "    return df\n",
    "df = clean_text(df, \"text\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3391754 words total, with a vocabulary size of 46322\n",
      "Max episode length is 36769\n",
      "Mean episode length is 15074.462222222222\n"
     ]
    }
   ],
   "source": [
    "# Print out some stats\n",
    "regexp_tokenizer = RegexpTokenizer(r'\\w+')\n",
    "word_tokens = df[\"text\"].apply(regexp_tokenizer.tokenize)\n",
    "\n",
    "# Inspecting our dataset a little more\n",
    "all_words = [word for tokens in word_tokens for word in tokens]\n",
    "sentence_lengths = [len(tokens) for tokens in word_tokens]\n",
    "VOCAB = sorted(list(set(all_words)))\n",
    "print(\"%s words total, with a vocabulary size of %s\" % (len(all_words), len(VOCAB)))\n",
    "print(\"Max episode length is %s\" % max(sentence_lengths))\n",
    "print(\"Mean episode length is %s\" % np.mean(sentence_lengths))\n",
    "\n",
    "label = df[\"title\"].tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First findings\n",
    "This data set is large enough to do some sierous NLP. Dealing with a very long document size will ne challenge. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Topic Modeling and Extraction\n",
    "To get a first impression this section will be doing some topic modeling and extraction.\n",
    "Applying a custome list of stop words is important here. \n",
    "A seperated python code will be load into this notebnook to remove these stopwords.\n",
    "\n",
    "#### Experiments\n",
    "* Used different number of topics (4, 5, 6)\n",
    "* Tried different parameter in each model\n",
    "\n",
    "#### NNFM\n",
    "* Set alpha from 0 to 0.1 --> much better results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 0:\n",
      "            LSA          LDA         NNMF\n",
      "0  people 31.98  people 0.35  people 8.19\n",
      "0   think 29.44   think 0.32   think 7.66\n",
      "0     one 26.74     one 0.29     one 6.76\n",
      "0    time 19.67    time 0.21    time 5.01\n",
      "0    know 17.53    know 0.19    know 4.49\n",
      "0  things 17.42  things 0.19  things 4.49\n",
      "0     say 16.63     say 0.18     say 4.26\n",
      "0      go 15.25    want 0.17      go 3.89\n",
      "0    want 15.23      go 0.17    want 3.87\n",
      "0     lot 14.52     lot 0.16     lot 3.74\n",
      "Topic 1:\n",
      "              LSA          LDA            NNMF\n",
      "1       diet 1.11  people 0.35  ketogenic 0.96\n",
      "1  ketogenic 1.05   think 0.32       diet 0.81\n",
      "1    training 0.8     one 0.29    ketones 0.41\n",
      "1       body 0.65    time 0.21    ketosis 0.36\n",
      "1    glucose 0.56    know 0.19    glucose 0.35\n",
      "1      coach 0.53  things 0.19     ketone 0.33\n",
      "1    ketones 0.52     say 0.18      think 0.28\n",
      "1     muscle 0.52    want 0.17     oxygen 0.27\n",
      "1   strength 0.51      go 0.17       beta 0.23\n",
      "1    ketosis 0.47     lot 0.16        one 0.21\n",
      "Topic 2:\n",
      "             LSA           LDA           NNMF\n",
      "2       got 0.82  people 32.44     coach 0.85\n",
      "2        go 0.77   think 29.75    people 0.36\n",
      "2      know 0.62     one 27.27       one 0.28\n",
      "2     coach 0.59    time 19.98  strength 0.27\n",
      "2       guy 0.46    know 17.78  training 0.25\n",
      "2       jiu 0.44  things 17.63      back 0.25\n",
      "2  training 0.43     say 16.87  mobility 0.23\n",
      "2      back 0.43    want 15.51      body 0.23\n",
      "2     jitsu 0.41      go 15.51        go 0.22\n",
      "2      kind 0.36     lot 14.76   shoulder 0.2\n",
      "Topic 3:\n",
      "                  LSA          LDA                NNMF\n",
      "3   psychedelics 0.65  people 0.35   psychedelics 0.56\n",
      "3    psychedelic 0.49   think 0.32         people 0.52\n",
      "3          think 0.48     one 0.29    psychedelic 0.44\n",
      "3     meditation 0.47    time 0.21          think 0.42\n",
      "3  consciousness 0.44    know 0.19            one 0.39\n",
      "3     experience 0.44  things 0.19     psilocybin 0.32\n",
      "3             dr 0.44     say 0.18     experience 0.32\n",
      "3     psilocybin 0.38    want 0.17             lsd 0.3\n",
      "3           mate 0.38      go 0.17          drugs 0.26\n",
      "3      ketogenic 0.37     lot 0.16  consciousness 0.25\n"
     ]
    }
   ],
   "source": [
    "from custom_stopwords import remove_custom_stopwords\n",
    "\n",
    "# Parameters to set:\n",
    "# Number of topics.\n",
    "ntopics=4\n",
    "# Number of words to look at for each topic.\n",
    "n_top_words = 10\n",
    "\n",
    "# Creating a dataframe to store the topic extraction results\n",
    "topwords=pd.DataFrame()\n",
    "\n",
    "# copy dataframe to convert the transcript to lower case\n",
    "df_topic = df.copy()\n",
    "df_topic['text'] = df_topic['text'].str.lower()\n",
    "\n",
    "# Creating the tf-idf matrix.\n",
    "vectorizer = TfidfVectorizer(stop_words=set(stopwords.words('english')), min_df=3, norm='l2')\n",
    "podcasts_tfidf = vectorizer.fit_transform(remove_custom_stopwords(df_topic, 'text').text)\n",
    "\n",
    "# Getting the word list.\n",
    "terms = vectorizer.get_feature_names()\n",
    "\n",
    "# Linking words to topics\n",
    "def word_topic(tfidf,solution, wordlist):\n",
    "    \n",
    "    # Loading scores for each word on each topic/component.\n",
    "    words_by_topic=tfidf.T * solution\n",
    "\n",
    "    # Linking the loadings to the words in an easy-to-read way.\n",
    "    components=pd.DataFrame(words_by_topic,index=wordlist)\n",
    "    \n",
    "    return components\n",
    "\n",
    "# Extracts the top N words and their loadings for each topic.\n",
    "def top_words(components, n_top_words):\n",
    "    topwords=pd.Series()\n",
    "    for column in range(components.shape[1]):\n",
    "        # Sort the column so that highest loadings are at the top.\n",
    "        sortedwords=components.iloc[:,column].sort_values(ascending=False)\n",
    "        # Choose the N highest loadings.\n",
    "        chosen=sortedwords[:n_top_words]\n",
    "        #print(chosen)\n",
    "        # Combine loading and index into a string.\n",
    "        for i, data in enumerate(chosen):\n",
    "            chosenlist=chosen.index.values.tolist()[i] +\" \"+ str(round(data ,2))\n",
    "            topwords = topwords.append(pd.Series([chosenlist], index=[column]))\n",
    "    return(topwords)\n",
    "\n",
    "# LSA\n",
    "svd = TruncatedSVD(n_components=ntopics)\n",
    "lsa_pipe = make_pipeline(svd, Normalizer(copy=False))\n",
    "podcasts_lsa = lsa_pipe.fit_transform(podcasts_tfidf)\n",
    "\n",
    "components_lsa = word_topic(podcasts_tfidf, podcasts_lsa, terms)\n",
    "\n",
    "topwords['LSA'] = top_words(components_lsa, n_top_words) \n",
    "\n",
    "# LDA\n",
    "from sklearn.decomposition import LatentDirichletAllocation as LDA\n",
    "\n",
    "lda = LDA(n_components=ntopics, \n",
    "          doc_topic_prior=1/ntopics,\n",
    "          topic_word_prior=1/ntopics,\n",
    "          learning_decay=0.6, # Convergence rate.\n",
    "          learning_offset=5.0, # Causes earlier iterations to have less influence on the learning\n",
    "          max_iter=500, # when to stop even if the model is not converging (to prevent running forever)\n",
    "          evaluate_every=3, # Do not evaluate perplexity, as it slows training time.\n",
    "          mean_change_tol=0.001, # Stop updating the document topic distribution in the E-step when mean change is < tol\n",
    "          max_doc_update_iter=100, # When to stop updating the document topic distribution in the E-step even if tol is not reached\n",
    "          n_jobs=-1, # Use all available CPUs to speed up processing time.\n",
    "          verbose=0, # amount of output to give while iterating\n",
    "          random_state=32\n",
    "         )\n",
    "\n",
    "podcasts_lda = lda.fit_transform(podcasts_tfidf) \n",
    "\n",
    "components_lda = word_topic(podcasts_tfidf, podcasts_lda, terms)\n",
    "\n",
    "topwords['LDA']=top_words(components_lda, n_top_words)\n",
    "\n",
    "# NNMF\n",
    "\n",
    "from sklearn.decomposition import NMF\n",
    "\n",
    "nmf = NMF(alpha=0.1, \n",
    "          init='nndsvdar', # how starting value are calculated\n",
    "          l1_ratio=0.5, # Sets whether regularization is L2 (0), L1 (1), or a combination (values between 0 and 1)\n",
    "          max_iter=400, # when to stop even if the model is not converging (to prevent running forever)\n",
    "          n_components=ntopics, \n",
    "          random_state=65, \n",
    "          solver='cd', # Use Coordinate Descent to solve\n",
    "          tol=0.0001, # model will stop if tfidf-WH <= tol\n",
    "          verbose=0 # amount of output to give while iterating\n",
    "         )\n",
    "podcasts_nmf = nmf.fit_transform(podcasts_tfidf) \n",
    "\n",
    "components_nmf = word_topic(podcasts_tfidf, podcasts_nmf, terms)\n",
    "\n",
    "topwords['NNMF']=top_words(components_nmf, n_top_words)\n",
    "\n",
    "#Show topics\n",
    "for topic in range(ntopics):\n",
    "    print('Topic {}:'.format(topic))\n",
    "    print(topwords.loc[topic])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Findings\n",
    "* 4 topics have the best consistency troughout the two models LSA and NNFM. They show similar top words.\n",
    "* 4 topics also make sense if you consider the words semantics\n",
    "* LDA performance poorly. It contains the same top words in each topic\n",
    "* NNFM performanc the best. Here the top words are better related to each other than in LSA.\n",
    "\n",
    "#### Found topics:\n",
    "1. Ketogenic diet\n",
    "2. Training\n",
    "3. Psychedelics\n",
    "4. All other"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenize with pre-trained language model\n",
    "It is time to use current advances in NLP by using common transformer architectures pre-trained on a large corpus of text like from the Wikipedia. \n",
    "### Experiments\n",
    "* Used Distill Bert, Bert, XLNet\n",
    "* uncased and cased \n",
    "* Clustering (number of cluster is 4, like the best number of topics, which was a finding of the topic modeling part)\n",
    "* metric: silhouette_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load distil BERT Model\n",
    "nlp = spacy.load('/usr/local/lib/python3.7/site-packages/en_pytt_xlnetbasecased_lg/en_pytt_xlnetbasecased_lg-2.1.1')\n",
    "n_items = 86 # Enough data for this show case\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vectorize transcripts with spacy  \n",
    "def get_embeddings_with_spacy(tokenizer, df):\n",
    "    embeddings = df['text'].apply(lambda x: tokenizer(x).vector)\n",
    "    return list(embeddings)\n",
    "\n",
    "spacy_transformer_embeddings = get_embeddings_with_spacy(nlp, df[:n_items])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dataframe\n",
    "df_emb_spacy = pd.DataFrame(spacy_transformer_embeddings)\n",
    "\n",
    "# Normalize \n",
    "scaler = MinMaxScaler() \n",
    "scaled_values = scaler.fit_transform(df_emb_spacy) \n",
    "df_emb_spacy.loc[:,:] = scaled_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clustering \n",
    "\n",
    "# Calculate predicted values.\n",
    "km = KMeans(n_clusters=4, random_state=42).fit(df_emb_spacy)\n",
    "y_pred = km.predict(df_emb_spacy)\n",
    "\n",
    "print('silhouette score: ', metrics.silhouette_score(df_emb_spacy, y_pred, metric='euclidean'))\n",
    "\n",
    "# 2D\n",
    "lsa = PCA(n_components=2)\n",
    "las_results = lsa.fit_transform(df_emb_spacy.values)\n",
    "las_results = pd.DataFrame(las_results, columns=['x', 'y'])\n",
    "\n",
    "df_y = pd.DataFrame(y_pred, columns=['y_pred'])\n",
    "df_y['y_pred'] = df_y['y_pred'].astype(int)\n",
    "\n",
    "las_results = pd.concat([las_results, df_y], axis=1)\n",
    "\n",
    "#Plot\n",
    "sns.set_style(\"white\")\n",
    "plt.figure(figsize=(10, 13))  \n",
    "fig = sns.scatterplot(x=las_results['x'], y=las_results['y'], hue=las_results['y_pred'])\n",
    "for i, txt in enumerate(label[:n_items]):\n",
    "    fig.annotate(txt, # this is the text\n",
    "                 (las_results['x'].values[i],las_results['y'].values[i]), # this is the point to label\n",
    "                 textcoords=\"offset points\", # how to position the text\n",
    "                 xytext=(0,3), # distance from text to points (x,y)\n",
    "                 ha='left', # horizontal alignment can be left, right or center\n",
    "                 size=3)\n",
    "fig.set_title(\"KMeans Cluster - Fig. 1\", fontsize=12)\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Findings:\n",
    "WIth more and more clusters the silhouette score is going down. So it is not going to help if you increase the cluster number or regarding the topic modeling part to increase the number of topics.\n",
    "With this data set it also is very hard to find clusters in generell. It looks like all the topics of the Tim Ferris show are very diverse and seamlessly complete each other. There is no obvious way to divide them. Looks like the clusters have a similar variance. That is why k-Means is not working well here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question and Answering (QnA\n",
    "### Builing a Tim Ferriss Bot\n",
    "The goal is to build a question and answering system with the podcast transcript corpus.\n",
    "This project is going to use cdQA, an End-To-End Closed Domain Question Answering System Framework: https://cdqa-suite.github.io/cdQA-website/\n",
    "\n",
    "Because we are only using these transcripts and we are only think about questions which should be get an answer out of this data set this QnA system will be closed domain one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create paragraphes\n",
    "# Use the interview structure and split it when Tim Ferriss starts to aks a question\n",
    "# If the podcast transcript has no structered interview then remove it from the data set\n",
    "# The treshold is 7. That means we coulnd split the transcript into 7 different pieces. \n",
    "df['paragraphs'] = df['text'].apply(lambda x: x.split(\"''Tim Ferriss:\") if len(x.split(\"''Tim Ferriss:\")) > 7 else None)\n",
    "\n",
    "# Define X\n",
    "df_X = df.drop(columns=['text', 'id']).dropna()\n",
    "df_X.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# Generate squad v1.1 json file so you can create annotated question and answers to fine-tune the model and evaluate it\n",
    "from cdqa.utils.converters import df2squad\n",
    "json_data = df2squad(df=df_X, squad_version='v1.1', output_dir='.', filename='qna_tim_ferriss')\n",
    "# From there we can use https://github.com/cdqa-suite/cdQA-annotator to create a supervised problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fine-Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fine-tune Bert model with squad v1.1 custom data set of Tim Ferriss questions\n",
    "import os\n",
    "import torch\n",
    "from sklearn.externals import joblib\n",
    "from cdqa.reader.bertqa_sklearn import BertProcessor, BertQA\n",
    "\n",
    "train_processor = BertProcessor(do_lower_case=True, is_training=True)\n",
    "train_examples, train_features = train_processor.fit_transform(X='cdqa-v1.1-tim_qna.json')\n",
    "\n",
    "reader = BertQA(train_batch_size=12,\n",
    "                learning_rate=3e-5,\n",
    "                num_train_epochs=2,\n",
    "                do_lower_case=True,\n",
    "                output_dir='models')\n",
    "\n",
    "reader.fit(X=(train_examples, train_features))\n",
    "\n",
    "# Output fine-tuned model\n",
    "reader.model.to('cpu')\n",
    "reader.device = torch.device('cpu')\n",
    "joblib.dump(reader, os.path.join(reader.output_dir, 'bert_tim_qa_vCPU.joblib'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cdqa.pipeline.cdqa_sklearn import QAPipeline\n",
    "\n",
    "# Load standard model\n",
    "cdqa_pipeline = QAPipeline(model='./cdqa/bert_qa_vCPU-sklearn.joblib', max_answer_length=60)\n",
    "cdqa_pipeline.fit_retriever(X=df_X)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate QnA system\n",
    "from cdqa.utils.evaluation import evaluate_pipeline\n",
    "evaluate_pipeline(cdqa_pipeline, 'cdqa-v1.1-tim_qna.json')\n",
    "\n",
    "# Standard pre trained model: {'exact_match': 0.0, 'f1': 5.025362668068075}\n",
    "# Fine-tuned model: {'exact_match': 0.0, 'f1': 5.684362620078064}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = cdqa_pipeline.predict(X='what would be a good gymnastic strength training goal to have?')\n",
    "print('title: {}'.format(prediction[1]))\n",
    "print('paragraph: {}'.format(prediction[2]))\n",
    "print('answer: {}'.format(prediction[0]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Findings:\n",
    "* Fine-tuning results in worse performance\n",
    "* F1 score is reall low --> not a good QnA system\n",
    "* Inferene is very slow on mediocre hardware\n",
    "* Inference results are also bad\n",
    "\n",
    "### Identifed issues\n",
    "* too many paragraphes --> intial choice on the document to find the anwwer is too hard with tf-idf\n",
    "* too many paragraphes with too short text, bad for tf-idf and to predict the answer\n",
    "* too slow hardware\n",
    "* fine tuning the QnA model is not working --> to less labled examples\n",
    "* even the paraprashes defgered from an interview text, Tim Ferriss is not asking a lot of question\n",
    "* in this podcast they are more telling stories than asking and anwering questions --> this makes it harder to find QnA pairs and to learn from the corpus."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "This text corpus is way more challenging than expected. Topic modeling and extraction works quite well, but using pre-trained models and state of the art transformer architectues have not lived up the praise. Result are far away to think about operationalizing the model. \n",
    "\n",
    "#### Things to do next:\n",
    "* Fine tune transformer model and clustering, also for QnA model\n",
    "* Use a transformer model for topic modeling and extraction\n",
    "* Get more QnA pairs\n",
    "* Get rid of unhelpful paragraphes for the QnA model\n",
    "* Use FARM as an alternative framework to build a QnA model\n",
    "* Build an API\n",
    "* Build a Telegram Chatbot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Miscellaneous"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fix issue where the id is missing\n",
    "import uuid\n",
    "import json\n",
    "\n",
    "with open('cdqa-v1.1-xxxx.json') as json_file:\n",
    "    annotated_dataset = json.load(json_file)\n",
    "\n",
    "for doc in annotated_dataset['data']:\n",
    "    for p in doc['paragraphs']:\n",
    "        for qa in p['qas']:\n",
    "            qa['id'] = str(uuid.uuid4())\n",
    "\n",
    "with open('cdqa-v1.1-tim_qna.json', 'w') as outfile:\n",
    "    json.dump(annotated_dataset, outfile)\n"
   ]
  }
 ],
 "metadata": {
  "file_extension": ".py",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
